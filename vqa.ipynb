{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##SSHing into GitHub Repo."
      ],
      "metadata": {
        "id": "4UaG1fklvHWw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6_uwAFuj4DO",
        "outputId": "0113f440-9601-450b-b36f-5ed028fb859a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating public/private rsa key pair.\n",
            "Enter file in which to save the key (/root/.ssh/id_rsa): \n",
            "Created directory '/root/.ssh'.\n",
            "Enter passphrase (empty for no passphrase): \n",
            "\n",
            "Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa.\n",
            "Your public key has been saved in /root/.ssh/id_rsa.pub.\n",
            "The key fingerprint is:\n",
            "SHA256:IoxgK4FtAGlaU3BxKoacOJ8R3Zs5UUbwOfqBCIqN0m0 root@db0bf835778a\n",
            "The key's randomart image is:\n",
            "+---[RSA 2048]----+\n",
            "|+.o++oo++        |\n",
            "|==+o.ooo .       |\n",
            "|X=B..  =+        |\n",
            "|+%.O .=o .       |\n",
            "|*.* E +.S        |\n",
            "|o  . . o .       |\n",
            "|        .        |\n",
            "|                 |\n",
            "|                 |\n",
            "+----[SHA256]-----+\n",
            "\n",
            "# github.com:22 SSH-2.0-babeld-181fb29f\n",
            "\u001b[H\u001b[2JPUBLIC KEY: (for github.com)\n",
            "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDC8t5VdHswG25OgVXpNvXhbMj4qxGLu8B/ocFlEcHJ2gWqt0F9l1kP9EUJIvzc6Pi3j86VT0oMXjGjEblk/J9GxX2SJIrnjhWGJXDtWcCzc9YnJVCNw7LK0Ae74Jmy67YY9mKjllGhoJ4bchR7rI7Jih1/VkcuGcQmhRhSn7ppR0ehLC4VTzbRjsGlxUVl/tL7Cb3nZsSu/6VIee/iG5PzDQeFKsQvjlllZYPdO1aqY0KHFXriWgLG7dUqdpr+3o67UXD29/lMe/gCWIJ3FOinppKdQEjuou8okiAiFQxkZjOXq4QRVECYL0Y/Yyc1w7Eej03Y8XrQRiUpjYjk4TVb root@db0bf835778a\n"
          ]
        }
      ],
      "source": [
        "!ssh-keygen -t rsa\n",
        "!ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts\n",
        "!clear\n",
        "\n",
        "!echo \"PUBLIC KEY: (for github.com)\"\n",
        "!cat /root/.ssh/id_rsa.pub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4ROzZpP2u06",
        "outputId": "bcb4831c-d14b-48e5-8678-60ab2d3c9e34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Cloning into 'dl-final-project'...\n",
            "Warning: Permanently added the RSA host key for IP address '20.205.243.166' to the list of known hosts.\n",
            "remote: Enumerating objects: 2023, done.\u001b[K\n",
            "remote: Counting objects: 100% (557/557), done.\u001b[K\n",
            "remote: Compressing objects: 100% (188/188), done.\u001b[K\n",
            "remote: Total 2023 (delta 363), reused 556 (delta 362), pack-reused 1466\u001b[K\n",
            "Receiving objects: 100% (2023/2023), 417.49 MiB | 7.97 MiB/s, done.\n",
            "Resolving deltas: 100% (366/366), done.\n",
            "Checking out files: 100% (1512/1512), done.\n",
            "/content/dl-final-project\n",
            "\u001b[0m\u001b[01;34mbaseline\u001b[0m/  \u001b[01;34mdataset\u001b[0m/  \u001b[01;34mpersonal\u001b[0m/  README.md  \u001b[01;34mtransformer\u001b[0m/  \u001b[01;34mtwochannel\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        " !git clone git@github.com:/jean-yoo/dl-final-project.git\n",
        " %cd dl-final-project\n",
        " %ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVL_EVzoj5ry",
        "outputId": "d32c4b2e-fe83-40da-aec3-70f0ecf59142"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Already up to date.\n",
            "\u001b[0m\u001b[01;34mbaseline\u001b[0m/  \u001b[01;34mdataset\u001b[0m/  \u001b[01;34mpersonal\u001b[0m/  README.md  \u001b[01;34mtransformer\u001b[0m/  \u001b[01;34mtwochannel\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "!git pull\n",
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing required packages in Colab Environment."
      ],
      "metadata": {
        "id": "07n5vrQavRFa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UyW288-kLh4",
        "outputId": "e0ee079d-8a5a-4946-ae3a-7c9fba8db5e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 31.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 79.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 66.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.7.1-py3-none-any.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 32.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.11.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.23.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 75.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 80.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 79.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: urllib3, xxhash, responses, multiprocess, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.7.1 multiprocess-0.70.14 responses-0.18.0 urllib3-1.25.11 xxhash-3.1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMdbh_tZkS8-",
        "outputId": "afa32cae-3b5d-4226-c972-8fd5c435d3e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.7.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.1.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.11.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNOT3p8CkTie",
        "outputId": "ecb53bd6-4ddf-4afc-cf9d-631441f9251b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from copy import deepcopy\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from datasets import load_dataset, set_caching_enabled\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "import torch.nn as nn\n",
        "\n",
        "print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cs7pSY3cqegh",
        "outputId": "172eec04-fb44-48bc-804f-3bbcc7e9d0b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Training For 10 Epochs + Inference on Two-Channel Vision + LM. \n",
        "- The image-question pairs are ordered in the same way for the two-channel and the transformer (i.e. first question asked for two-channel = first question asked for transformer, and so on). "
      ],
      "metadata": {
        "id": "1C6IUQJfvW2c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9W87UOauXn8",
        "outputId": "c7a7f90c-5d15-4c58-85ca-fff76d51b25e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using custom data configuration default-96ff2f39695ee44b\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-96ff2f39695ee44b/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 10305.42it/s]\n",
            "\n",
            "Extracting data files #1: 100% 1/1 [00:00<00:00, 300.15obj/s]\n",
            "Extracting data files #0: 100% 1/1 [00:00<00:00, 309.59obj/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-96ff2f39695ee44b/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 755.87it/s]\n",
            "Using custom data configuration default-96ff2f39695ee44b\n",
            "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-96ff2f39695ee44b/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
            "100% 2/2 [00:00<00:00, 1048.05it/s]\n",
            "100% 10/10 [00:00<00:00, 125.56ba/s]\n",
            "100% 3/3 [00:00<00:00, 175.34ba/s]\n",
            "100% 10/10 [00:00<00:00, 40.86ba/s]\n",
            "100% 3/3 [00:00<00:00, 50.06ba/s]\n",
            "100% 10/10 [02:00<00:00, 12.04s/ba]\n",
            "100% 3/3 [00:28<00:00,  9.57s/ba]\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100% 548M/548M [00:02<00:00, 286MB/s]\n",
            "train() called: model=TwoChanNN, opt=Adam(lr=0.001000), epochs=10, device=cuda\n",
            "\n",
            "381\n",
            "Epoch   1/ 10, train loss:  0.16, train acc:  0.04, train wups: 0.08, val wups: 0.07, val loss:  0.16, val acc:  0.03\n",
            "651\n",
            "Epoch   2/ 10, train loss:  0.15, train acc:  0.06, train wups: 0.12, val wups: 0.13, val loss:  0.15, val acc:  0.07\n",
            "724\n",
            "Epoch   3/ 10, train loss:  0.14, train acc:  0.07, train wups: 0.13, val wups: 0.13, val loss:  0.14, val acc:  0.07\n",
            "848\n",
            "Epoch   4/ 10, train loss:  0.14, train acc:  0.08, train wups: 0.14, val wups: 0.17, val loss:  0.14, val acc:  0.10\n",
            "1171\n",
            "Epoch   5/ 10, train loss:  0.13, train acc:  0.11, train wups: 0.18, val wups: 0.18, val loss:  0.13, val acc:  0.11\n",
            "1372\n",
            "Epoch   6/ 10, train loss:  0.12, train acc:  0.13, train wups: 0.19, val wups: 0.20, val loss:  0.13, val acc:  0.13\n",
            "1659\n",
            "Epoch   7/ 10, train loss:  0.12, train acc:  0.16, train wups: 0.22, val wups: 0.24, val loss:  0.12, val acc:  0.17\n",
            "2008\n",
            "Epoch   8/ 10, train loss:  0.11, train acc:  0.19, train wups: 0.25, val wups: 0.26, val loss:  0.12, val acc:  0.19\n",
            "2244\n",
            "Epoch   9/ 10, train loss:  0.10, train acc:  0.21, train wups: 0.28, val wups: 0.27, val loss:  0.12, val acc:  0.20\n",
            "2439\n",
            "Epoch  10/ 10, train loss:  0.10, train acc:  0.23, train wups: 0.29, val wups: 0.27, val loss:  0.12, val acc:  0.21\n",
            "\n",
            "Time total:     637.67 sec\n",
            "Time per epoch: 63.77 sec\n",
            "basket\n",
            "window\n",
            "2\n",
            "table\n",
            "table\n",
            "shelves\n",
            "sofa\n",
            "sofa\n",
            "chair\n",
            "dvd_player\n",
            "glass\n",
            "pillow\n",
            "blanket\n",
            "table\n",
            "brown\n",
            "2\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!python3 twochannel/run_two_channel.py #--extractor model_ft --task train --feat_size num_ftrs --device device --epochs 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Training For 12 Epochs + Inference on DAQUAR Test Set with Transformer Model."
      ],
      "metadata": {
        "id": "6m35yfYYvfMS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJsu6QDHkDat",
        "outputId": "c49acb75-ee52-4016-f3c4-b10714c801c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/dl-final-project/transformer/preprocess.py:28: FutureWarning: set_caching_enabled is deprecated and will be removed in the next major version of datasets. Use datasets.enable_caching() or datasets.disable_caching() instead. This function will be removed in a future version of datasets.\n",
            "  set_caching_enabled(True)\n",
            "cuda\n",
            "WARNING:datasets.builder:Using custom data configuration default-5ee86db0e7b42765\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-5ee86db0e7b42765/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 8507.72it/s]\n",
            "Extracting data files #0:   0% 0/1 [00:00<?, ?obj/s]\n",
            "Extracting data files #1: 100% 1/1 [00:00<00:00, 410.60obj/s]\n",
            "Extracting data files #0: 100% 1/1 [00:00<00:00, 137.09obj/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-5ee86db0e7b42765/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 607.25it/s]\n",
            "100% 10/10 [00:00<00:00, 59.52ba/s]\n",
            "100% 3/3 [00:00<00:00, 96.67ba/s]\n",
            "Downloading: 100% 481/481 [00:00<00:00, 343kB/s]\n",
            "Downloading: 100% 899k/899k [00:01<00:00, 681kB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 507kB/s]\n",
            "Downloading: 100% 1.36M/1.36M [00:01<00:00, 1.02MB/s]\n",
            "Downloading: 100% 160/160 [00:00<00:00, 145kB/s]\n",
            "Downloading: 100% 502/502 [00:00<00:00, 464kB/s]\n",
            "Downloading: 100% 501M/501M [00:05<00:00, 99.1MB/s]\n",
            "Downloading: 100% 346M/346M [00:03<00:00, 99.4MB/s]\n",
            "Using cuda_amp half precision backend\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 9974\n",
            "  Num Epochs = 12\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 3744\n",
            "  Number of trainable parameters = 212120390\n",
            "{'loss': 5.5633, 'learning_rate': 4.8664529914529914e-05, 'epoch': 0.32}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 4.793865203857422, 'eval_wups': 0.18661130006993745, 'eval_acc': 0.11507618283881316, 'eval_f1': 0.003234625758244339, 'eval_runtime': 41.3347, 'eval_samples_per_second': 60.337, 'eval_steps_per_second': 1.887, 'epoch': 0.32}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-100\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "{'loss': 4.5712, 'learning_rate': 4.7355769230769234e-05, 'epoch': 0.64}\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 4.345043182373047, 'eval_wups': 0.20384878353850633, 'eval_acc': 0.14354450681635927, 'eval_f1': 0.006988411898380376, 'eval_runtime': 41.0925, 'eval_samples_per_second': 60.692, 'eval_steps_per_second': 1.898, 'epoch': 0.64}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-200\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "{'loss': 4.2768, 'learning_rate': 4.6020299145299145e-05, 'epoch': 0.96}\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 4.135875225067139, 'eval_wups': 0.22899536536787218, 'eval_acc': 0.17361668003207698, 'eval_f1': 0.00984269356371926, 'eval_runtime': 42.7874, 'eval_samples_per_second': 58.288, 'eval_steps_per_second': 1.823, 'epoch': 0.96}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-300\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "{'loss': 4.0386, 'learning_rate': 4.468482905982906e-05, 'epoch': 1.28}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.9607224464416504, 'eval_wups': 0.23965044213628353, 'eval_acc': 0.1904570970328789, 'eval_f1': 0.01449556875010277, 'eval_runtime': 42.6349, 'eval_samples_per_second': 58.497, 'eval_steps_per_second': 1.829, 'epoch': 1.28}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-400\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-100] due to args.save_total_limit\n",
            "{'loss': 3.9448, 'learning_rate': 4.334935897435898e-05, 'epoch': 1.6}\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.834055185317993, 'eval_wups': 0.26479994716264155, 'eval_acc': 0.2141138732959102, 'eval_f1': 0.016551711813576963, 'eval_runtime': 40.2869, 'eval_samples_per_second': 61.906, 'eval_steps_per_second': 1.936, 'epoch': 1.6}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-500\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-200] due to args.save_total_limit\n",
            "{'loss': 3.7502, 'learning_rate': 4.201388888888889e-05, 'epoch': 1.92}\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.7586660385131836, 'eval_wups': 0.265684808518075, 'eval_acc': 0.2141138732959102, 'eval_f1': 0.01857548070866924, 'eval_runtime': 45.7097, 'eval_samples_per_second': 54.562, 'eval_steps_per_second': 1.706, 'epoch': 1.92}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-600\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-300] due to args.save_total_limit\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "{'loss': 3.5714, 'learning_rate': 4.06784188034188e-05, 'epoch': 2.24}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.6585867404937744, 'eval_wups': 0.27808887806191473, 'eval_acc': 0.22373696872493987, 'eval_f1': 0.0205930412427891, 'eval_runtime': 39.8757, 'eval_samples_per_second': 62.544, 'eval_steps_per_second': 1.956, 'epoch': 2.24}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-700\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-400] due to args.save_total_limit\n",
            "{'loss': 3.5178, 'learning_rate': 3.934294871794872e-05, 'epoch': 2.56}\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.5723936557769775, 'eval_wups': 0.286813300484662, 'eval_acc': 0.23857257417802727, 'eval_f1': 0.025720315576759387, 'eval_runtime': 40.0871, 'eval_samples_per_second': 62.215, 'eval_steps_per_second': 1.946, 'epoch': 2.56}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-800\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-500] due to args.save_total_limit\n",
            "{'loss': 3.4653, 'learning_rate': 3.800747863247863e-05, 'epoch': 2.88}\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.527254819869995, 'eval_wups': 0.29585311403663994, 'eval_acc': 0.24779470729751404, 'eval_f1': 0.02892217092442086, 'eval_runtime': 38.5529, 'eval_samples_per_second': 64.69, 'eval_steps_per_second': 2.023, 'epoch': 2.88}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-900\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-600] due to args.save_total_limit\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "{'loss': 3.3115, 'learning_rate': 3.6672008547008544e-05, 'epoch': 3.21}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.50005841255188, 'eval_wups': 0.2909743907592581, 'eval_acc': 0.2421812349639134, 'eval_f1': 0.028319561960110595, 'eval_runtime': 40.308, 'eval_samples_per_second': 61.874, 'eval_steps_per_second': 1.935, 'epoch': 3.21}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-1000\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-700] due to args.save_total_limit\n",
            "{'loss': 3.252, 'learning_rate': 3.533653846153847e-05, 'epoch': 3.53}\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.4598207473754883, 'eval_wups': 0.29714433050270617, 'eval_acc': 0.24899759422614273, 'eval_f1': 0.0330284766538644, 'eval_runtime': 38.5022, 'eval_samples_per_second': 64.776, 'eval_steps_per_second': 2.026, 'epoch': 3.53}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-1100\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-800] due to args.save_total_limit\n",
            "{'loss': 3.1744, 'learning_rate': 3.400106837606838e-05, 'epoch': 3.85}\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.4073643684387207, 'eval_wups': 0.30025284271264285, 'eval_acc': 0.25340817963111467, 'eval_f1': 0.03433647091818687, 'eval_runtime': 44.3155, 'eval_samples_per_second': 56.278, 'eval_steps_per_second': 1.76, 'epoch': 3.85}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-1200\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-900] due to args.save_total_limit\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "{'loss': 3.035, 'learning_rate': 3.266559829059829e-05, 'epoch': 4.17}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.389540433883667, 'eval_wups': 0.30814675842925576, 'eval_acc': 0.2594226142742582, 'eval_f1': 0.037474418905498456, 'eval_runtime': 40.4452, 'eval_samples_per_second': 61.664, 'eval_steps_per_second': 1.929, 'epoch': 4.17}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-1300\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-1000] due to args.save_total_limit\n",
            "{'loss': 2.943, 'learning_rate': 3.133012820512821e-05, 'epoch': 4.49}\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.3751115798950195, 'eval_wups': 0.31185166921925533, 'eval_acc': 0.264635124298316, 'eval_f1': 0.04366770942441177, 'eval_runtime': 40.0239, 'eval_samples_per_second': 62.313, 'eval_steps_per_second': 1.949, 'epoch': 4.49}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-1400\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-1100] due to args.save_total_limit\n",
            "{'loss': 2.9934, 'learning_rate': 2.999465811965812e-05, 'epoch': 4.81}\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.338390350341797, 'eval_wups': 0.3050539605347706, 'eval_acc': 0.25821972734562953, 'eval_f1': 0.04194040588248859, 'eval_runtime': 40.2327, 'eval_samples_per_second': 61.989, 'eval_steps_per_second': 1.939, 'epoch': 4.81}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-1500\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-1200] due to args.save_total_limit\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "{'loss': 2.8983, 'learning_rate': 2.8659188034188034e-05, 'epoch': 5.13}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.3058571815490723, 'eval_wups': 0.3151928725676165, 'eval_acc': 0.2678428227746592, 'eval_f1': 0.0460652192143237, 'eval_runtime': 38.2443, 'eval_samples_per_second': 65.212, 'eval_steps_per_second': 2.04, 'epoch': 5.13}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-1600\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-1300] due to args.save_total_limit\n",
            "{'loss': 2.7959, 'learning_rate': 2.732371794871795e-05, 'epoch': 5.45}\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.3020174503326416, 'eval_wups': 0.32264409835689384, 'eval_acc': 0.27586206896551724, 'eval_f1': 0.0497365416520242, 'eval_runtime': 38.3993, 'eval_samples_per_second': 64.949, 'eval_steps_per_second': 2.031, 'epoch': 5.45}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-1700\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-1400] due to args.save_total_limit\n",
            "{'loss': 2.7059, 'learning_rate': 2.5988247863247867e-05, 'epoch': 5.77}\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.2980873584747314, 'eval_wups': 0.32655240838469657, 'eval_acc': 0.27866880513231757, 'eval_f1': 0.052802145035813416, 'eval_runtime': 38.1559, 'eval_samples_per_second': 65.363, 'eval_steps_per_second': 2.044, 'epoch': 5.77}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-1800\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-1500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "{'loss': 2.6469, 'learning_rate': 2.465277777777778e-05, 'epoch': 6.09}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.287893772125244, 'eval_wups': 0.3251511981127569, 'eval_acc': 0.27866880513231757, 'eval_f1': 0.054728198048527715, 'eval_runtime': 39.6129, 'eval_samples_per_second': 62.959, 'eval_steps_per_second': 1.969, 'epoch': 6.09}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-1900\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-1600] due to args.save_total_limit\n",
            "{'loss': 2.5411, 'learning_rate': 2.3317307692307692e-05, 'epoch': 6.41}\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.282543420791626, 'eval_wups': 0.3214999416559717, 'eval_acc': 0.27666399358460303, 'eval_f1': 0.05276006175474508, 'eval_runtime': 41.5613, 'eval_samples_per_second': 60.008, 'eval_steps_per_second': 1.877, 'epoch': 6.41}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-2000\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-1700] due to args.save_total_limit\n",
            "{'loss': 2.6022, 'learning_rate': 2.1981837606837607e-05, 'epoch': 6.73}\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.2600698471069336, 'eval_wups': 0.32741343210442964, 'eval_acc': 0.2814755412991179, 'eval_f1': 0.05826429113430831, 'eval_runtime': 40.9879, 'eval_samples_per_second': 60.847, 'eval_steps_per_second': 1.903, 'epoch': 6.73}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-2100\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-1800] due to args.save_total_limit\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "{'loss': 2.5165, 'learning_rate': 2.064636752136752e-05, 'epoch': 7.05}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.2520248889923096, 'eval_wups': 0.3322406685427113, 'eval_acc': 0.2862870890136327, 'eval_f1': 0.06062006170078128, 'eval_runtime': 37.978, 'eval_samples_per_second': 65.67, 'eval_steps_per_second': 2.054, 'epoch': 7.05}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-2200\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-1900] due to args.save_total_limit\n",
            "{'loss': 2.3716, 'learning_rate': 1.931089743589744e-05, 'epoch': 7.37}\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.256459951400757, 'eval_wups': 0.3357749549507542, 'eval_acc': 0.29109863672814756, 'eval_f1': 0.0615604390653774, 'eval_runtime': 39.8762, 'eval_samples_per_second': 62.544, 'eval_steps_per_second': 1.956, 'epoch': 7.37}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-2300\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-2000] due to args.save_total_limit\n",
            "{'loss': 2.4105, 'learning_rate': 1.797542735042735e-05, 'epoch': 7.69}\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.2417619228363037, 'eval_wups': 0.33654156966081666, 'eval_acc': 0.29230152365677625, 'eval_f1': 0.06384642402488035, 'eval_runtime': 37.5508, 'eval_samples_per_second': 66.417, 'eval_steps_per_second': 2.077, 'epoch': 7.69}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-2400\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-2100] due to args.save_total_limit\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "{'loss': 2.4174, 'learning_rate': 1.6639957264957265e-05, 'epoch': 8.01}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.2383337020874023, 'eval_wups': 0.33562027214352075, 'eval_acc': 0.29109863672814756, 'eval_f1': 0.06435531538052085, 'eval_runtime': 39.0602, 'eval_samples_per_second': 63.85, 'eval_steps_per_second': 1.997, 'epoch': 8.01}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-2500\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-2200] due to args.save_total_limit\n",
            "{'loss': 2.2761, 'learning_rate': 1.5304487179487183e-05, 'epoch': 8.33}\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.238126516342163, 'eval_wups': 0.3430274903358813, 'eval_acc': 0.2987169206094627, 'eval_f1': 0.07187616526105536, 'eval_runtime': 37.8724, 'eval_samples_per_second': 65.853, 'eval_steps_per_second': 2.06, 'epoch': 8.33}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-2600\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-2300] due to args.save_total_limit\n",
            "{'loss': 2.2394, 'learning_rate': 1.3969017094017096e-05, 'epoch': 8.65}\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.2455193996429443, 'eval_wups': 0.3382571227893587, 'eval_acc': 0.293504410585405, 'eval_f1': 0.07000442562673201, 'eval_runtime': 40.7319, 'eval_samples_per_second': 61.23, 'eval_steps_per_second': 1.915, 'epoch': 8.65}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-2700\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-2400] due to args.save_total_limit\n",
            "{'loss': 2.2461, 'learning_rate': 1.2633547008547008e-05, 'epoch': 8.97}\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.229374408721924, 'eval_wups': 0.34082620176806516, 'eval_acc': 0.29591018444266237, 'eval_f1': 0.06801549298628573, 'eval_runtime': 40.258, 'eval_samples_per_second': 61.95, 'eval_steps_per_second': 1.938, 'epoch': 8.97}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-2800\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-2500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "{'loss': 2.1429, 'learning_rate': 1.1298076923076923e-05, 'epoch': 9.29}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.239917278289795, 'eval_wups': 0.3413212620438414, 'eval_acc': 0.29631114675220527, 'eval_f1': 0.07113423190439701, 'eval_runtime': 42.315, 'eval_samples_per_second': 58.939, 'eval_steps_per_second': 1.843, 'epoch': 9.29}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-2900\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-2700] due to args.save_total_limit\n",
            "{'loss': 2.1389, 'learning_rate': 9.96260683760684e-06, 'epoch': 9.62}\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.2393994331359863, 'eval_wups': 0.34206634366043887, 'eval_acc': 0.297514033680834, 'eval_f1': 0.07264131856467686, 'eval_runtime': 39.1496, 'eval_samples_per_second': 63.704, 'eval_steps_per_second': 1.992, 'epoch': 9.62}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-3000\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-2800] due to args.save_total_limit\n",
            "{'loss': 2.1423, 'learning_rate': 8.627136752136752e-06, 'epoch': 9.94}\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.2375283241271973, 'eval_wups': 0.34200617356601853, 'eval_acc': 0.2987169206094627, 'eval_f1': 0.07538054071258594, 'eval_runtime': 41.7468, 'eval_samples_per_second': 59.741, 'eval_steps_per_second': 1.868, 'epoch': 9.94}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-3100\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-2900] due to args.save_total_limit\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "{'loss': 2.0294, 'learning_rate': 7.2916666666666674e-06, 'epoch': 10.26}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.241896629333496, 'eval_wups': 0.33697985851298695, 'eval_acc': 0.29230152365677625, 'eval_f1': 0.06924838275497905, 'eval_runtime': 37.7293, 'eval_samples_per_second': 66.103, 'eval_steps_per_second': 2.067, 'epoch': 10.26}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-3200\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-3000] due to args.save_total_limit\n",
            "{'loss': 2.054, 'learning_rate': 5.956196581196581e-06, 'epoch': 10.58}\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.2312633991241455, 'eval_wups': 0.34242931174934466, 'eval_acc': 0.2991178829190056, 'eval_f1': 0.0705644607651089, 'eval_runtime': 38.131, 'eval_samples_per_second': 65.406, 'eval_steps_per_second': 2.046, 'epoch': 10.58}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-3300\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-3100] due to args.save_total_limit\n",
            "{'loss': 2.0447, 'learning_rate': 4.6207264957264965e-06, 'epoch': 10.9}\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.2448604106903076, 'eval_wups': 0.3478981077366304, 'eval_acc': 0.30392943063352046, 'eval_f1': 0.07483098605176913, 'eval_runtime': 38.542, 'eval_samples_per_second': 64.709, 'eval_steps_per_second': 2.024, 'epoch': 10.9}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-3400\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-2600] due to args.save_total_limit\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "{'loss': 1.939, 'learning_rate': 3.2852564102564106e-06, 'epoch': 11.22}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.25545597076416, 'eval_wups': 0.3436592952131898, 'eval_acc': 0.2987169206094627, 'eval_f1': 0.07290691822403533, 'eval_runtime': 41.4318, 'eval_samples_per_second': 60.195, 'eval_steps_per_second': 1.883, 'epoch': 11.22}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-3500\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-3200] due to args.save_total_limit\n",
            "{'loss': 1.9775, 'learning_rate': 1.9497863247863247e-06, 'epoch': 11.54}\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.248250961303711, 'eval_wups': 0.3428553002052104, 'eval_acc': 0.297514033680834, 'eval_f1': 0.07361244556425785, 'eval_runtime': 37.7967, 'eval_samples_per_second': 65.985, 'eval_steps_per_second': 2.064, 'epoch': 11.54}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-3600\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-3300] due to args.save_total_limit\n",
            "{'loss': 1.974, 'learning_rate': 6.143162393162394e-07, 'epoch': 11.86}\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.250756025314331, 'eval_wups': 0.34685246885413146, 'eval_acc': 0.3019246190858059, 'eval_f1': 0.07474420978213543, 'eval_runtime': 37.6388, 'eval_samples_per_second': 66.261, 'eval_steps_per_second': 2.072, 'epoch': 11.86}\n",
            "Saving model checkpoint to /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-3700\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Deleting older checkpoint [/Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-3500] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /Users/jean/Documents/CS1470/dl-final-project/transformer/transformer-checkpoints/checkpoint-3400 (score: 0.3478981077366304).\n",
            "{'train_runtime': 4205.2306, 'train_samples_per_second': 28.462, 'train_steps_per_second': 0.89, 'train_loss': 2.868492336354704, 'epoch': 12.0}\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2494\n",
            "  Batch size = 32\n",
            "{'eval_loss': 3.2448604106903076, 'eval_wups': 0.3478981077366304, 'eval_acc': 0.30392943063352046, 'eval_f1': 0.07483098605176913, 'eval_runtime': 39.8158, 'eval_samples_per_second': 62.639, 'eval_steps_per_second': 1.959, 'epoch': 12.0}\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "Figure(640x480)\n",
            "Question:\t what is below the board\n",
            "Answer:\t\t hooks\n",
            "Prediction:\t table\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "Figure(640x480)\n",
            "Question:\t what is behind the sofa\n",
            "Answer:\t\t table\n",
            "Prediction:\t window\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "Figure(640x480)\n",
            "Question:\t how many sofas are there\n",
            "Answer:\t\t 2\n",
            "Prediction:\t 2\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "Figure(640x480)\n",
            "Question:\t what is found below the window\n",
            "Answer:\t\t treadmill\n",
            "Prediction:\t sofa\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "Figure(640x480)\n",
            "Question:\t what is to the right of the bookshelf\n",
            "Answer:\t\t bookrack\n",
            "Prediction:\t bookshelf\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "Figure(640x480)\n",
            "Question:\t what is to right of door\n",
            "Answer:\t\t toilet\n",
            "Prediction:\t sink\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "Figure(640x480)\n",
            "Question:\t what is at the bottom of the photo\n",
            "Answer:\t\t stool\n",
            "Prediction:\t bed\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "Figure(640x480)\n",
            "Question:\t what is the largest object\n",
            "Answer:\t\t sofa\n",
            "Prediction:\t sofa\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "Figure(640x480)\n",
            "Question:\t what is to the left of the shelf\n",
            "Answer:\t\t plastic_box\n",
            "Prediction:\t sofa\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "Figure(640x480)\n",
            "Question:\t what is in front of the television monitor\n",
            "Answer:\t\t bottle_of_liquid, dvd_player\n",
            "Prediction:\t towel\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "Traceback (most recent call last):\n",
            "  File \"transformer/run_model.py\", line 99, in <module>\n",
            "    model_personal_inference(model, collator)\n",
            "  File \"transformer/run_model.py\", line 87, in model_personal_inference\n",
            "    sample = collator({'answer': ['pillow'], 'image_id': ['personal-couch'], 'label': torch.tensor([384]), 'question': ['what is on the couch']})\n",
            "  File \"/content/dl-final-project/transformer/preprocess.py\", line 108, in __call__\n",
            "    **self.preprocess_images(\n",
            "  File \"/content/dl-final-project/transformer/preprocess.py\", line 93, in preprocess_images\n",
            "    else [Image.open(os.path.join(\"dataset\", \"images\", image_id + \".png\")).convert('RGB') for image_id in images],\n",
            "  File \"/content/dl-final-project/transformer/preprocess.py\", line 93, in <listcomp>\n",
            "    else [Image.open(os.path.join(\"dataset\", \"images\", image_id + \".png\")).convert('RGB') for image_id in images],\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/PIL/Image.py\", line 2843, in open\n",
            "    fp = builtins.open(filename, \"rb\")\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'dataset/images/personal-couch.png'\n"
          ]
        }
      ],
      "source": [
        "!python3 transformer/run_model.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ran into errors during inference on our personal dataset because of a typo in the file, so ran a version of the .py file that just loads the training weights from the last checkpoint and re-does the inference. "
      ],
      "metadata": {
        "id": "SIWtUZZ1vlqm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFV_JBMjvh72",
        "outputId": "525ab0e6-8d9e-42ee-9c2c-3bfcffb0172e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/dl-final-project/dl-final-project/transformer/preprocess.py:28: FutureWarning: set_caching_enabled is deprecated and will be removed in the next major version of datasets. Use datasets.enable_caching() or datasets.disable_caching() instead. This function will be removed in a future version of datasets.\n",
            "  set_caching_enabled(True)\n",
            "cuda\n",
            "WARNING:datasets.builder:Using custom data configuration default-715c5018d10f1bb0\n",
            "WARNING:datasets.builder:Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-715c5018d10f1bb0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
            "100% 2/2 [00:00<00:00, 478.09it/s]\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-715c5018d10f1bb0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-61240bf4b0842166.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-715c5018d10f1bb0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6d3013932818e029.arrow\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "Figure(640x480)\n",
            "Question:\t what is below the board\n",
            "Answer:\t\t hooks\n",
            "Prediction:\t table\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "Figure(640x480)\n",
            "Question:\t what is behind the sofa\n",
            "Answer:\t\t table\n",
            "Prediction:\t window\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "Figure(640x480)\n",
            "Question:\t how many sofas are there\n",
            "Answer:\t\t 2\n",
            "Prediction:\t 2\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "Figure(640x480)\n",
            "Question:\t what is found below the window\n",
            "Answer:\t\t treadmill\n",
            "Prediction:\t sofa\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "Figure(640x480)\n",
            "Question:\t what is to the right of the bookshelf\n",
            "Answer:\t\t bookrack\n",
            "Prediction:\t bookshelf\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "Figure(640x480)\n",
            "Question:\t what is to right of door\n",
            "Answer:\t\t toilet\n",
            "Prediction:\t sink\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "Figure(640x480)\n",
            "Question:\t what is at the bottom of the photo\n",
            "Answer:\t\t stool\n",
            "Prediction:\t bed\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "Figure(640x480)\n",
            "Question:\t what is the largest object\n",
            "Answer:\t\t sofa\n",
            "Prediction:\t sofa\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "Figure(640x480)\n",
            "Question:\t what is to the left of the shelf\n",
            "Answer:\t\t plastic_box\n",
            "Prediction:\t sofa\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "Figure(640x480)\n",
            "Question:\t what is in front of the television monitor\n",
            "Answer:\t\t bottle_of_liquid, dvd_player\n",
            "Prediction:\t towel\n",
            "$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
            "/content/dl-final-project/dl-final-project/transformer/preprocess.py:113: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'labels': torch.tensor(\n",
            "keyboard\n",
            "pillow\n",
            "glass\n",
            "table\n",
            "brown\n",
            "2\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "!python3 transformer/run_model.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}